{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb38bc10-ad1a-4701-8bfc-98c6dd78c5b3",
   "metadata": {},
   "source": [
    "# CLUSTERING - armado de las temáticas de inmigración\n",
    "\n",
    "\n",
    "Este Notebook se encarga del armado de las temáticas de inmigración, siguiendo estos pasos:\n",
    "\n",
    "1. [Matrices $L2V$, distancia coseno, $TFIDF$](#matrices)\n",
    "2. [Fast K-Medoids + Random Forest](#clusters-rf)\n",
    "    - [Para distintos valores de $K$](#clusters-rf-k)\n",
    "    - [10 repeticiones para $K$ = 500](#clusters-rf-10)\n",
    "    - [Clúster más predictor](#clusters-rf-1)\n",
    "    - [Comparar particiones y definir $T$ clústers más predictores](#clusters-rf-t)\n",
    "3. [Reagrupación vía clustering jerárquico](#jerarq)\n",
    "4. [Etiquetas finales](#etiquetas)\n",
    "\n",
    "\n",
    "Inputs:\n",
    "- Vocabulario único (stoi.pkl)\n",
    "- Dataset a nivel de película (master_subt_content_cleaned_lite.pkl\") con las siguientes variables:\n",
    "    - tconst (string): identificador de película\n",
    "    - final_lemmas (lista de strings): lemas únicos de cada película\n",
    "    - just_migra (int): variable indicadora de si una película es de inmigración o no\n",
    "    \n",
    "Outputs:\n",
    "- Matrices $L2V$, $TFIDF$ y filmids\n",
    "- Dataset con las temáticas de inmigración final_clusters500.pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ad3c2-ed79-4fa3-a6f5-4837e71b4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías \n",
    "\n",
    "## Módulos generales\n",
    "from libraries import *\n",
    "\n",
    "## Módulos con funciones creadas para este trabajo\n",
    "## (requieren de haber importado previamente los módulos generales)\n",
    "from limpieza_subt import *\n",
    "from clustering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ecfdc3-4e17-489d-9d1c-0e8078f75a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completar con directorios \n",
    "gitwd = \"\"\n",
    "datawd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92747f83-63da-4739-9922-d2b0aedbef9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a802d8-a27e-463c-995e-6eaefbc32204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open stoi\n",
    "with open(datawd + \"/stoi.pkl\", 'rb') as inputfile: \n",
    "    stoi = pickle.load(inputfile)\n",
    "UNK_IDX = stoi[\"<unk>\"]\n",
    "\n",
    "# by film dataset\n",
    "master = pd.read_pickle(datawd + \"/master_subt_content_cleaned_lite.pkl\")  \n",
    "byfilm = master[ (master.in_cleaned == 1) ].reset_index(drop = True)\n",
    "byfilm = byfilm[(byfilm.main + byfilm.before2000 + byfilm.just_migra) > 0].reset_index(drop = True)\n",
    "byfilm = byfilm[[\"tconst\", \"final_lemmas\" ,\"just_migra\"]]\n",
    "del(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780725a9-c9da-474c-8b2d-717c161c1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "byfilm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa73e3-7ecb-4a15-a432-cd2b868caf2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='matrices'></a>\n",
    "## Matrices L2V, distancia coseno, TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346b132-998b-49ef-a852-d9b9c9c08cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### L2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e3955-7c9d-44c1-a474-154ddb32b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset con vocabulario único\n",
    "l2v = pd.DataFrame.from_dict(stoi, orient='index').reset_index()\n",
    "l2v.columns = [\"lemma\", \"stoi\"]\n",
    "l2v = l2v[~l2v.lemma.isin([\"<unk>\",\"<pad>\"])]\n",
    "l2v.reset_index(drop = True, inplace = True)\n",
    "l2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17126b05-ffef-464b-b6ca-276adfeb5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener vectores de cada lema usando Glove\n",
    "model = api.load(\"glove-wiki-gigaword-300\")  # load glove vectors\n",
    "\n",
    "l2v[\"WORD_VECTORS\"] = [get_word_vector(word, model) for word in tqdm(l2v.lemma)]\n",
    "df_transformed = l2v.WORD_VECTORS.apply(pd.Series)\n",
    "df_transformed.columns = [f'dim_{i+1}' for i in range(len(df_transformed.columns))]\n",
    "l2v = pd.concat([l2v[[\"lemma\",\"stoi\"]], df_transformed ], axis = 1)\n",
    "del(df_transformed)\n",
    "del(model)\n",
    "\n",
    "# Guardar\n",
    "l2v.to_pickle(datawd + \"/l2v.pkl\")\n",
    "l2v.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fabf00-e043-450e-ac6e-671be1f5125a",
   "metadata": {},
   "source": [
    "### Distancia coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b4910-06b5-4a1d-8a3c-7cdfe636f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de similaridad coseno\n",
    "dims = [c for c in l2v.columns if \"dim_\" in c]\n",
    "cosine_sim_matrix = cosine_similarity(l2v[dims], l2v[dims])\n",
    "\n",
    "# Valor mínimo y máximo en la matriz y reescalamiento min-max\n",
    "min_value = np.min(cosine_sim_matrix)\n",
    "max_value = np.max(cosine_sim_matrix)\n",
    "\n",
    "cosine_sim_matrix = (cosine_sim_matrix - min_value) / (max_value - min_value)\n",
    "\n",
    "# De similaridad a distancia (PAM requiere matriz de distancias)\n",
    "distance_matrix = 1 - cosine_sim_matrix\n",
    "del(cosine_sim_matrix)\n",
    "\n",
    "# save\n",
    "with open(datawd + \"/d.pkl\", 'wb') as outputfile: \n",
    "    pickle.dump(distance_matrix, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dae372-3f49-4c12-9a34-56499623e302",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2be42-a086-461a-a1fd-60b66a5505ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) Generar matriz de cuenta de lemas --------------------------------------------------------\n",
    "counts = byfilm[[\"tconst\", \"final_lemmas\"]].explode(\"final_lemmas\").reset_index(drop = True)\n",
    "counts[\"aux\"] = 1\n",
    "counts = counts.groupby([\"tconst\", \"final_lemmas\"], as_index = False).agg({\"aux\": sum})  # sum by tconst - lemma\n",
    "counts[\"aux\"] = counts.aux.astype(\"int\")\n",
    "counts.reset_index(inplace = True, drop = True)\n",
    "counts[\"stoi\"] = counts['final_lemmas'].map(stoi) \n",
    "print(min(counts.stoi), max(counts.stoi)) \n",
    "\n",
    "# Crear índices para guardar cuenta de lemas en una matriz esparsa (de lo contrario, no entra en memoria): IDs de cada película (tconst) asignados a un número (guardarlos en filmids)\n",
    "counts[\"tconst\"] = counts[\"tconst\"].astype('category') # 28519 categories\n",
    "counts[\"filmid\"] =  counts[\"tconst\"].cat.codes\n",
    "\n",
    "filmids = counts[[\"tconst\", \"filmid\"]].drop_duplicates().reset_index(drop = True)\n",
    "filmids[\"tconst\"] = filmids.tconst.astype(\"str\")\n",
    "filmids = filmids.merge(byfilm, \n",
    "                        how = \"left\",\n",
    "                        on = \"tconst\")\n",
    "\n",
    "filmids.to_pickle(datawd + \"/filmids.pkl\")\n",
    "del(byfilm)\n",
    "\n",
    "\n",
    "# Armar matriz esparsa de cuenta cuenta de lemas en cada película\n",
    "sparse_matrix = sp.coo_matrix( ( counts['aux'], (counts[\"filmid\"], counts['stoi'])) )\n",
    "del counts\n",
    "sparse_matrix  # 27018 columns because the maximum is 27017 but we also have 0 (whihc is empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ced341-7a79-467c-8180-2a5b06db71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(sparse_matrix.getcol(i).getnnz()) #  las columnas 0 y 1 están vacías porque corresponden a los tokens UNK y PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e44a9-f9b3-43f5-8831-7fb72c5a3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B) Generar matriz TFIDF ----------------------------------------------------------\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(sparse_matrix)\n",
    "del(transformer, sparse_matrix)\n",
    "tfidf\n",
    "\n",
    " # save\n",
    "with open(datawd + \"/tfidf.pkl\", 'wb') as outputfile: \n",
    "    pickle.dump(tfidf, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29706e-07c4-4b61-abe9-ea2da3602899",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='clusters-rf'></a>\n",
    "## Fast K-Medoids + Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23904b-328a-4bb2-8ff2-16290cbb7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necesary datasets: l2v, distance_matrix, filmids, tfidf\n",
    "filmids = pd.read_pickle(datawd + \"/filmids.pkl\")\n",
    "\n",
    "with open(datawd + \"/tfidf.pkl\", 'rb') as inputfile: \n",
    "    tfidf = pickle.load(inputfile)\n",
    "    \n",
    "l2v = pd.read_pickle(datawd + \"/l2v.pkl\")\n",
    "\n",
    "with open(datawd + \"/d.pkl\", 'rb') as inputfile: \n",
    "    distance_matrix = pickle.load(inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979befa-7075-40d3-99e7-cb22b1d550d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='clusters-rf-k'></a>\n",
    "### 1. Para distintos K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b43ca-3696-4143-bb3c-e47c2e6d59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = []\n",
    "roc_aucs = []\n",
    "n_lemmas = []\n",
    "n_important = []\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "for k in tqdm([50, 250, 500, 1000]): \n",
    "\n",
    "   \n",
    "    seed(42)\n",
    "    a = Clusters_RF(l2v, distance_matrix, filmids, tfidf,\n",
    "                    k, rseed = 42)\n",
    "    a.get_clusters()\n",
    "    a.describe_clusters()\n",
    "    a.get_silhouette()\n",
    "    a.get_f2c()\n",
    "    a.rf()\n",
    "    \n",
    "    \n",
    "    n_i = np.sum(a.feature_importances.Importance > 0)\n",
    "    print(f\"Clusters con importancia mayor a 0: {n_i}\")\n",
    "    n_important.append(n_i)\n",
    "    \n",
    "    silhouettes.append(a.silhouette)\n",
    "    roc_aucs.append(a.roc_auc)\n",
    "    n_lemmas.append(np.mean(a.clusters.n_lemmas))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b0fd2-4a2c-4230-97df-1ef1ae86843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(silhouettes,\n",
    "      roc_aucs,\n",
    "      n_lemmas,\n",
    "     n_important)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c219fe-43b3-4ce7-a5e6-6e9ae5f2998d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='clusters-rf-10'></a>\n",
    "### 2. Para k = 500, 10 repeticiones con distinta semilla para los clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acac1a7-5874-4b02-85eb-4ac8242fa81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = []\n",
    "roc_aucs = []\n",
    "n_important_clusters = []\n",
    "n_lemmas = []\n",
    "cluster_importances_k = pd.DataFrame()\n",
    "ntop = 25\n",
    "k = 500\n",
    "rseed = 42\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    seed(rseed)\n",
    "    a = Clusters_RF(l2v, distance_matrix, filmids, tfidf,\n",
    "                    k, rseed)\n",
    "    a.get_clusters()\n",
    "    a.describe_clusters()\n",
    "    a.get_silhouette()\n",
    "    a.get_f2c()\n",
    "    a.rf()\n",
    "    \n",
    "    silhouettes.append(a.silhouette)\n",
    "    roc_aucs.append(a.roc_auc)\n",
    "    n_lemmas.append(np.mean(a.clusters.n_lemmas))\n",
    "\n",
    "    cluster_importances = a.feature_importances.merge(a.clusters,\n",
    "                                                    how = \"left\",\n",
    "                                                    on = \"cluster\")\n",
    "\n",
    "    cluster_importances[\"t\"] = i\n",
    "\n",
    "    # keep just the important clusters\n",
    "    n_important_clusters.append( np.sum(cluster_importances.Importance > 0) )\n",
    "    cluster_importances = cluster_importances[cluster_importances.Importance > 0].reset_index(drop = True)\n",
    "\n",
    "\n",
    "    cluster_importances_k = pd.concat([cluster_importances_k, cluster_importances], axis = 0)\n",
    "\n",
    "    rseed +=1  # cambia la semilla!\n",
    "\n",
    "\n",
    "cluster_importances_k.to_pickle(datawd + f\"/clusters{k}/cluster_importances_{k}.pkl\") \n",
    "print(\"K =\", k)\n",
    "print(\"mean silhouettes:\", silhouettes)\n",
    "print(\"mean roc_aucs:\", roc_aucs)\n",
    "print(\"mean n_lemmas:\", np.mean(n_lemmas))\n",
    "print(\"n_important_clusters:\", n_important_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da96913-9062-45f8-969c-3f19b35f5d9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='clusters-rf-1'></a>\n",
    "### 3. Clúster más predictor de inmigración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e233a-06a1-4956-860d-104556cfd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "clusters = pd.read_pickle(datawd + f\"/clusters{k}/cluster_importances_{k}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457125f-45bb-44ee-b3c7-f3604c359db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop = 1\n",
    "top = clusters.groupby(\"t\").head(ntop).sort_values([\"t\", \"Cumulative\"]).reset_index(drop = True)\n",
    "top[\"Importance_order\"] = top.groupby(\"t\")['Importance'].rank(ascending=False).astype(int)\n",
    "[x for x in top.lemmas]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66857ed3-628e-4b92-8fd2-e6487a1fbe7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='clusters-rf-t'></a>\n",
    "### 4. Comparar particiones y seleccionar T clústers más predictores en cada repetición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93588c6a-3fc1-4ba2-ba05-0d46f23d9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_pickle(datawd + f\"/clusters500/cluster_importances_500.pkl\")\n",
    "ntop = 16\n",
    "top = clusters.groupby(\"t\").head(ntop).sort_values([\"t\", \"Cumulative\"]).reset_index(drop = True)\n",
    "top[\"Importance_order\"] = top.groupby(\"t\")['Importance'].rank(ascending=False).astype(int)\n",
    "top.head()\n",
    "\n",
    "\n",
    "# Comparamos las 10 particiones entre sí\n",
    "comparison = pd.DataFrame(columns = [\"P1\",\"P2\", \"cluster_P1\", \"cluster_P2\", \n",
    "                                     \"i_rank_P1\" , \"i_rank_P2\" ,\n",
    "                                     \"intersection\" , \"mean_cos_sim\" ] )\n",
    "\n",
    "partitions = top['t'].unique()\n",
    "\n",
    "# Iteramos a lo largo de todas las posibles combinaciones de 2 clústers, uno de cada partición\n",
    "r = 0\n",
    "for group1, group2 in tqdm(combinations(partitions, 2)):\n",
    "    group1_data = top[top['t'] == group1]\n",
    "    group2_data = top[top['t'] == group2]\n",
    "\n",
    "    for index1, row1 in group1_data.iterrows():\n",
    "        for index2, row2 in group2_data.iterrows():\n",
    "            comparison.loc[r, \"P1\"] = row1['t']\n",
    "            comparison.loc[r, \"P2\"] = row2['t']\n",
    "            comparison.loc[r, \"cluster_P1\"] = row1['cluster']\n",
    "            comparison.loc[r, \"cluster_P2\"] = row2['cluster']\n",
    "            comparison.loc[r, \"i_rank_P1\"] = row1['Importance_order']\n",
    "            comparison.loc[r, \"i_rank_P2\"] = row2['Importance_order']\n",
    "            comparison.loc[r, \"intersection\"] = prop_intersection(row1['lemmas'],  \n",
    "                                                                  row2['lemmas'])\n",
    "            comparison.loc[r, \"mean_cos_sim\"] = mean_cos_sim(row1['lemmas'], \n",
    "                                                             row2['lemmas'],\n",
    "                                                             l2v)\n",
    "\n",
    "\n",
    "            r = r + 1\n",
    "\n",
    "mi = np.min(comparison.mean_cos_sim)\n",
    "ma = np.max(comparison.mean_cos_sim)\n",
    "comparison[\"mean_cos_sim_01\"] = (comparison.mean_cos_sim - mi) / (ma - mi)\n",
    "comparison.to_pickle(datawd + f\"/clusters500/comparison_clusters500.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6e329-617d-4a19-bd4d-5649c37bc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos métricas para distintos valores de T\n",
    "clusters = pd.read_pickle(datawd + f\"/clusters500/cluster_importances_500.pkl\")\n",
    "top = clusters.groupby(\"t\").head(15).sort_values([\"t\", \"Cumulative\"]).reset_index(drop = True)\n",
    "top[\"Importance_order\"] = top.groupby(\"t\")['Importance'].rank(ascending=False).astype(int)\n",
    "\n",
    "comparison = pd.read_pickle(datawd + f\"/clusters500/comparison_clusters500.pkl\")\n",
    "\n",
    "t_top_metrics = pd.DataFrame()\n",
    "\n",
    "for T in tqdm(np.arange(1,16)):\n",
    "    importance = top[top.Importance_order <= T].Importance.sum()\n",
    "    intersection = comparison.loc[(comparison.i_rank_P1 <= T) & (comparison.i_rank_P2 <= T) , \"intersection\"].mean()\n",
    "    t = pd.DataFrame.from_dict({\"importance_sum\" : [importance] ,\n",
    "                                \"intersection_prop\" : [intersection] })\n",
    "    t_top_metrics  = pd.concat([t_top_metrics, t], axis = 0).reset_index(drop = True)\n",
    "\n",
    "t_top_metrics[\"T\"] = np.arange(1,16) \n",
    "\n",
    "# Nos quedamos con 10 clusters\n",
    "ntop = 10\n",
    "\n",
    "for c in t_top_metrics.columns[:-1]:\n",
    "    plt.scatter(t_top_metrics[\"T\"], t_top_metrics[c])\n",
    "    plt.axvline(x  = ntop, color = \"red\", alpha = 0.5)\n",
    "    plt.xlabel(\"T top clústers\")\n",
    "    plt.ylabel(c)\n",
    "    plt.ylim(ymin = 0)\n",
    "    plt.savefig(datawd + f\"/clusters{k}/top_clusters_{c}.png\", dpi = 300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb570d9-6869-40a7-a42f-22a50f0756db",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='jerarq'></a>\n",
    "## Reagrupación vía clustering jerárquico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099cad9-7f4a-4a05-b7af-416a6676a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k= 500\n",
    "clusters   = pd.read_pickle(datawd + f\"/clusters{k}/cluster_importances_{k}.pkl\")\n",
    "comparison = pd.read_pickle(datawd +  f\"/clusters{k}/comparison_clusters{k}.pkl\")\n",
    "ntop = 10\n",
    "\n",
    "top = clusters.groupby(\"t\").head(ntop).sort_values([\"t\", \"Cumulative\"]).reset_index(drop = True)\n",
    "top[\"Importance_order\"] = top.groupby(\"t\")['Importance'].rank(ascending=False).astype(int)\n",
    "comparison = comparison[(comparison.i_rank_P1 <= ntop) & (comparison.i_rank_P2 <= ntop)]\n",
    "comparison[\"c1\"] = (\"1\" + comparison.P1.astype(\"str\") + comparison.cluster_P1.astype(str)).astype(\"int\")\n",
    "comparison[\"c2\"] = (\"1\" + comparison.P2.astype(\"str\") + comparison.cluster_P2.astype(str)).astype(\"int\")\n",
    "cs = np.unique(pd.concat([comparison.c1, comparison.c2]))\n",
    "idmap = {cs[i]: i for i in range(len(cs))}\n",
    "\n",
    "comparison[\"c1id\"] = comparison.c1.map(idmap)\n",
    "comparison[\"c2id\"] = comparison.c2.map(idmap)\n",
    "\n",
    "top[\"c\"]  = (\"1\" + top.t.astype(\"str\") + top.cluster.astype(str)).astype(\"int\")\n",
    "top[\"cid\"] = top.c.map(idmap)\n",
    "\n",
    "\n",
    "seed(9)\n",
    "labs = pd.DataFrame()\n",
    "\n",
    "# hierarchical clustering\n",
    "for di in [\"intersection\", \"mean_cos_sim_01\"]:\n",
    "\n",
    "    # create distance matrix\n",
    "    a = comparison[[\"c1id\", \"c2id\", di]]\n",
    "    a.columns = [\"r\", \"c\" ,\"val\"]\n",
    "    b = comparison[[\"c2id\", \"c1id\", di]]\n",
    "    b.columns = [\"r\", \"c\" ,\"val\"]\n",
    "    print(a.shape)\n",
    "    aux = pd.concat([a, b], axis = 0).reset_index(drop = True)\n",
    "    del(a, b)\n",
    "    print(aux.shape)\n",
    "    aux[\"val\"] = aux.val.astype(\"float\")\n",
    "    distance_matrix = sp.coo_matrix( ( aux['val'], (aux[\"r\"], aux['c'])) ) ## sparse similarity\n",
    "    distance_matrix = (1 - distance_matrix.toarray()) ## to distance\n",
    "    np.fill_diagonal(distance_matrix, 0)\n",
    "    distance_matrix = squareform(distance_matrix)\n",
    "\n",
    "    for m in [\"complete\", \"single\"]:\n",
    "\n",
    "        if m + \"_\" + di !=  \"single_mean_cos_sim_01\":\n",
    "            th = 0.7\n",
    "        if m + \"_\" + di == \"single_mean_cos_sim_01\":\n",
    "            th = 0.45\n",
    "\n",
    "        l    = hierarchy.linkage(distance_matrix, method=m)  \n",
    "        labs[m + \"_\" + di] = hierarchy.fcluster(l, criterion='distance', t = th)\n",
    "\n",
    "\n",
    "        # Dendogramas\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        d = hierarchy.dendrogram(l, orientation='top', \n",
    "                                 labels = list(idmap.values()) ,\n",
    "                                 color_threshold = th )\n",
    "        plt.title(f\"{m}_{di}\")\n",
    "        plt.xlabel('Items')\n",
    "        plt.ylabel('Distancia')\n",
    "        plt.savefig(datawd + f\"/clusters{k}/dendogram_{m}_{di}.png\", dpi = 300)\n",
    "        plt.show()\n",
    "\n",
    "labs = labs.reset_index()\n",
    "labs.rename(columns = {\"index\" : \"cid\"}, inplace = True)\n",
    "\n",
    "# Agregar etiquetas\n",
    "top = top[['Importance', 'cluster', 'Cumulative', 'n_lemmas', 'lemmas', 't',\n",
    "       'Importance_order', 'c', 'cid']]\n",
    "\n",
    "top = top.merge(labs,\n",
    "                how = \"left\",\n",
    "                on = \"cid\")\n",
    "\n",
    "comparative_tab = top.groupby(\"complete_mean_cos_sim_01\", as_index = False).agg({\"single_intersection\"   : \"unique\",\n",
    "                                                               \"complete_intersection\" : \"unique\",\n",
    "                                                               \"single_mean_cos_sim_01\": \"unique\" })\n",
    "\n",
    "comparative_tab.to_excel(datawd + f\"/clusters{k}/clustering_clusters{k}_comparative.xlsx\")\n",
    "top.to_pickle(datawd + f\"/clusters{k}/clustering_clusters{k}_metrics.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d19063-bbe8-42bc-993d-c253a7a39fbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='etiquetas'></a>\n",
    "## Etiquetas finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ddb1f-d3f5-45ca-ac0e-1b2499737613",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=500\n",
    "top = pd.read_pickle(datawd + f\"/clusters{k}/clustering_clusters{k}_metrics.pkl\")\n",
    "top_final = top.groupby(\"complete_mean_cos_sim_01\", as_index = False).agg({\"lemmas\" : list})\n",
    "top_final[\"lemmas\"] = [np.unique([item for sublist in row for item in sublist]) for row in top_final.lemmas]\n",
    "\n",
    "manual_grouping = {   \"Trabajos bien pagos\" :5,\n",
    "                              \"Lugares del mundo\": 11,\n",
    "                              \"Lenguage\" : 3, \n",
    "                               \"Nazismo\": 9,\n",
    "                               \"Religión, ideología\": 6,\n",
    "                               \"Ley inmigratoria\": 8,\n",
    "                               \"Metafísica\": 6,\n",
    "                               \"Gentilicios\":11,\n",
    "                               \"Latino\" : 2, \n",
    "                               \"Lugares en Europa I\":  4,\n",
    "                               \"Lugares en Europa II\": 4 ,  \n",
    "                               \"Árabe israelí\": 7,\n",
    "                           \"Política Estados Unidos\": 1,\n",
    "                           \"Nueva York\" : 1, \n",
    "                            \"Británico\": 10,\n",
    "                            \"Lugares en Norte América\": 1, \n",
    "                            \"Ley inmigratoria II\": 8,\n",
    "                           \"Tecnología\": 12,\n",
    "                           \"Latino II\" : 2, \n",
    "                           \"Medio oriente\": 7,\n",
    "                           \"Economía comercial\": 5,\n",
    "                            \"Moneda\": 5}\n",
    "\n",
    "\n",
    "manual_grouping_names = {1: \"Nueva York y Estados Unidos\",\n",
    "                        2: \"Latino\",\n",
    "                        3: \"Lenguaje\",\n",
    "                        4: \"Europa\",\n",
    "                        5: \"Economía y empleo\",\n",
    "                        6: \"Religión, ideología, cosmovisión\",\n",
    "                        7: \"Conflictos medio oriente\",\n",
    "                        8: \"Ley inmigratoria\",\n",
    "                        9: \"Nazismo\",\n",
    "                        10: \"Británico\",\n",
    "                        11: \"Gentilicios y lugares del mundo\",\n",
    "                        12: \"Tecnología\"}\n",
    "\n",
    "\n",
    "top_final[\"cluster_name\"] = manual_grouping.keys()\n",
    "top_final[\"manual_grouping_id\"] =  top_final.cluster_name.map(manual_grouping)\n",
    "top_final[\"manual_grouping_name\"] = top_final.manual_grouping_id.map(manual_grouping_names)\n",
    "\n",
    "top_final.to_pickle(datawd + \"/clusters500/clustering_clusters_names.pkl\")\n",
    "\n",
    "top_final_manual = top_final.groupby(\"manual_grouping_name\", as_index = False).agg({\"lemmas\" : list})\n",
    "top_final_manual[\"lemmas\"] = [np.unique([item for sublist in row for item in sublist]) for row in top_final_manual.lemmas]\n",
    "top_final_manual[\"f_cluster_id\"] = np.arange(0, top_final_manual.shape[0])\n",
    "\n",
    "top_final_manual.to_pickle(datawd + \"/clusters500/final_clusters500.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d68b7-99db-48a4-9f71-f512ecc58971",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_final_manual[\"n_lemmas\"] = [len(x) for x in top_final_manual.lemmas]\n",
    "top_final_manual[[\"manual_grouping_name\", \"n_lemmas\", \"lemmas\"]].to_excel(datawd + \"/clusters500/final_clusters.xlsx\")\n",
    "top_final_manual.n_lemmas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd49933-aa79-4d52-a912-c3434c8ffc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_final_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac039f-f5e1-4641-9604-86c0b4c2a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras repetidas entre clusters\n",
    "words_multiple_clusters = top_final_manual.explode(\"lemmas\").groupby(\"lemmas\").agg({\"manual_grouping_name\": [\"nunique\", \"unique\"]})\n",
    "words_multiple_clusters.columns = [\"n\", \"clusters\"] \n",
    "print(words_multiple_clusters[words_multiple_clusters.n > 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
