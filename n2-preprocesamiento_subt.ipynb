{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocesamiento subtítulos\n",
    "\n",
    "Este Notebook se encarga de la limpieza y preprocesamiento inicial de los subtítulos. Partiendo del texto del subtítulo de cada película, se siguen estos pasos:\n",
    "\n",
    "1. [Quitar subtítulos que por error no son en inglés](#lang)\n",
    "2. [Quedarse con archivos .srt](#srt) (son el 99%)\n",
    "3. [Subtítulo a líneas](#subt_to_line)\n",
    "4. [Lematizar](#lemas)\n",
    "5. [Filtrar lemas y líneas](#filtro)\n",
    "    \n",
    "    \n",
    "Input:\n",
    "- Datasets con el texto del subtítulo de cada película: master_subt_content.pkl, con 3 variables centrales:\n",
    "    - tconst (string): identificador película\n",
    "    - s (string): texto del subtítulo\n",
    "    - subt (string): nombre del archivo de subtítulo\n",
    "\n",
    "Outputs:\n",
    "- Archivos con el vocabulario único (vocab, stoi)\n",
    "- Lemas limpios por película y por línea\n",
    "    - Por película: master_subt_content_cleaned_lite.pkl, aquellas películas con in_cleaned == 1\n",
    "    - Por línea: en archivos separados en una carpeta llamada linesdf_lemmas_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías \n",
    "\n",
    "## Módulos generales\n",
    "from libraries import *\n",
    "\n",
    "## Módulos con funciones creadas para este trabajo\n",
    "## (requieren de haber importado previamente los módulos generales)\n",
    "from limpieza_subt import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completar con directorios \n",
    "gitwd =  \"\"\n",
    "datawd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importar datos\n",
    "Archivo con el texto de los subtítulos para cada película: master_subt_content.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_subt_content = pd.read_pickle(datawd + \"/master_subt_content.pkl\")  \n",
    "print(master_subt_content.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_subt_content = master_subt_content[[\"tconst\", \"subt\", \"s\"]]\n",
    "master_subt_content.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='lang'></a>\n",
    "## Quitar subtítulos que por error no son en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = [langdetect(x) for x in tqdm(master_subt_content.s)]\n",
    "lang = pd.DataFrame(lang)\n",
    "lang.columns = [\"lang\", \"prob_lang\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver qué idiomas tenemos\n",
    "lang.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unimos al dataset de subtitulos para ver algunos ejemplos\n",
    "master_subt_content = pd.concat([master_subt_content, lang], axis = 1)\n",
    "master_subt_content.s[master_subt_content.lang == \"es\"].reset_index(drop = True)[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con las películas que son en inglés con mayor probabilidad\n",
    "master_subt_content = master_subt_content[master_subt_content.lang == \"en\"].reset_index(drop = True)\n",
    "master_subt_content.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos los casos de baja probabilidad asignada al inglés: estan OK, mezclan idiomas o tienen algunas palabras raras, pero las sacaremos más adelante en la limpieza\n",
    "master_subt_content.prob_lang.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_subt_content.prob_lang[master_subt_content.prob_lang < 0.7].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_subt_content.s[master_subt_content.prob_lang < 0.5].reset_index(drop = True)[0][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='srt'></a>\n",
    "## Agregar el tipo de archivo\n",
    "Por el momento nos quedaremos solamente con los archivos .srt, a futuro extender las funciones de limpieza a otro tipo de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add subtitles file type\n",
    "master_subt_content[\"file_type\"] = [fname[-4:] for fname in master_subt_content.subt]\n",
    "print(master_subt_content.file_type.value_counts())\n",
    "\n",
    "# for now keep just srt type of files, then extend cleaning functions to the other types [TBC]\n",
    "print(f\"Porcentaje de archivos tipo .srt: {round(np.mean([master_subt_content.file_type == '.srt']) * 100)} %\")\n",
    "\n",
    "master_subt_content = master_subt_content[master_subt_content.file_type == '.srt'].reset_index(drop = True)\n",
    "print(master_subt_content.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Guardar versión hasta aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_subt_content[[\"tconst\", \"s\"]].to_pickle(datawd + \"/master_subt_content_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='subt_to_line'></a>\n",
    "## Subtítulo a líneas\n",
    "\n",
    "Obtener una base de datos al nivel de línea con marcas de tiempo.\n",
    "\n",
    "Además: unidecode, lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_films = master_subt_content.shape[0]\n",
    "\n",
    "# Crear un dataset a nivel de línea llamado linesdf\n",
    "linesdf = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(n_films)):\n",
    "    \n",
    "    cleanlines = subtitle_to_lines(master_subt_content.s[i])\n",
    "    tconst =  master_subt_content.tconst[i]\n",
    "    if len(cleanlines) > 0 :\n",
    "        df = pd.DataFrame(cleanlines).reset_index() # index is the order of the line in the film\n",
    "        df.columns = ['line_number', 'time', 'line']\n",
    "        df['tconst'] = tconst\n",
    "        linesdf = pd.concat([linesdf, df])\n",
    "    \n",
    "    # guardar cada 1000 películas para no tener un dataset enorme\n",
    "    if i in np.arange(1000, n_films, 1000):\n",
    "        linesdf.reset_index(drop = True, inplace = True) # reset general index\n",
    "        linesdf.to_pickle(datawd + \"/linesdfs/linesdf_master_batch\" + str(i) + \".pkl\")   \n",
    "        linesdf = pd.DataFrame() # reset to empty dataset at the line level with cleaned lines\n",
    "        \n",
    "        \n",
    "linesdf.to_pickle(datawd + \"/linesdfs/linesdf_master_batch\" + str(i) + \".pkl\")   \n",
    "del(linesdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar los datasets pequeños y guardarlos en un sólo dataset\n",
    "linesdfs = os.listdir(datawd + \"/linesdfs\")\n",
    "print(linesdfs)\n",
    "linesdfs =[datawd + \"/linesdfs/\" + i for i in linesdfs]\n",
    "linesdf = pd.DataFrame()\n",
    "for i in tqdm(range(len(linesdfs))):\n",
    "    df = pd.read_pickle(linesdfs[i])\n",
    "    linesdf = pd.concat([linesdf, df])\n",
    "    \n",
    "linesdf.reset_index(drop = True, inplace = True)\n",
    "linesdf.to_pickle(datawd + \"/linesdf_master.pkl\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='lemas'></a>\n",
    "## Lematizar\n",
    "\n",
    "Usando un modelo de Spacy, tokenizamos, quitamos Stopwords, lematizamos cada línea del subtítulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesdf = pd.read_pickle(datawd + \"/linesdf_master.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linesdf.shape)\n",
    "seed(9)\n",
    "linesdf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar Spacy model ycustomizar configuración\n",
    "nlp = spacy.load(\"en_core_web_sm\", \n",
    "                 exclude=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]) \n",
    "\n",
    "nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"}).initialize() # lematizador usando look-up table\n",
    "\n",
    "# agregar stopwords customizadas\n",
    "nlp.Defaults.stop_words |= {'yeah', 'okay', 'yes', 'right', 'like', 'sure', \n",
    "                            'hey', 'hi', 'hello', \n",
    "                            'thing', \n",
    "                            'oh', 'huh', 'na', 'uh', 'ha', 'whoa', \n",
    "                            'ah', 'hmm', 'beep', 'uh', 'ah', 'wow', \n",
    "                            'way', 'um', 'ya', 'woaah', 'aha', 'ahem', \n",
    "                            'ahh', 'argh', 'aww', 'aw', 'bah', 'boo', \n",
    "                            'hoo', 'brr', 'duh', 'eek', 'eep', 'eh', \n",
    "                            'eww', 'fuff', 'gah', 'gee', 'grr', 'humph', \n",
    "                            'hah', 'haha', 'huh', 'hurrah', 'ick',\n",
    "                            'meh', 'mhm', 'mm', 'muahaha', 'mwah', \n",
    "                            'nah', 'nuh', 'uh', 'ooh', 'la', 'oomph', 'oops',\n",
    "                            'ouch', 'oww', 'oy', 'pew', 'pff', 'phew', 'psst', 'sheesh', \n",
    "                            'shh', 'shoo', 'tsk', 'umm', 'waah', 'wee', 'yahoo', 'yay',\n",
    "                            'yee', 'haw', 'yikes', 'yoo', 'hoo', 'yuh', 'uh', 'yuck', 'zing'}\n",
    "\n",
    "# customizar tokenización\n",
    "suffixes = nlp.Defaults.suffixes + [r'''-''',]\n",
    "suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n",
    "nlp.tokenizer.suffix_search = suffix_regex.search\n",
    "\n",
    "prefixes = nlp.Defaults.prefixes + [r'''-''',]\n",
    "prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n",
    "nlp.tokenizer.prefix_search = prefix_regex.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de lematización por línea\n",
    "example = \"--hey these (boys) and girls are dancing!!  !!!!!   !!!  \"\n",
    "doc = nlp(example)\n",
    "lemmas = [t.lemma_ for t in doc if (not t.is_stop) & (not t.is_punct) & (not t.is_space) & (t.is_ascii) ]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematizar todas las líneas\n",
    "counter = Counter() # ir contando la cantidad de apariciones de cada lema\n",
    "\n",
    "# de a subconjuntos del 4% de todos los datos (el proceso total lleva unas 7 horas)\n",
    "n = linesdf.shape[0]\n",
    "print(\"Total filas:\", n)\n",
    "step = int(np.floor(n/25))    \n",
    "indexes = np.arange(0 , n-step , step).tolist() +  [n]\n",
    "perc = 0\n",
    "\n",
    "for i in range(len(indexes)-1):\n",
    "    \n",
    "    perc += 4\n",
    "    \n",
    "    # Quedarse con el 4% of dataset\n",
    "    linesdf_lemmas = linesdf.iloc[ indexes[i]:  indexes[i+1], :].reset_index(drop = True)\n",
    "    \n",
    "    # Guardar columna con lemmas\n",
    "    linesdf_lemmas[\"lemmas\"] = np.nan\n",
    "    row_count = 0\n",
    "    \n",
    "    # Spacy model\n",
    "    docs = nlp.pipe(linesdf_lemmas.line, n_process=8, batch_size=2000) # procesamiento en paralelo\n",
    "    \n",
    "    for doc in tqdm(docs):\n",
    "    \n",
    "        # Tokenizar, quitar stopwords, quitar puntuación, lematizar\n",
    "        lemmas = [t.lemma_ for t in doc if (not t.is_stop) & (not t.is_punct) & (not t.is_space) & (t.is_ascii)]\n",
    "        \n",
    "        # Guardar lemas concatenados en una columna\n",
    "        linesdf_lemmas.loc[row_count,\"lemmas\"] = \" \".join(lemmas)\n",
    "        \n",
    "        row_count += 1\n",
    "        \n",
    "        # Actualizar counter\n",
    "        for lemma in lemmas:\n",
    "            counter[lemma] += 1 # cuenta cuántas veces aparece cada token\n",
    "    \n",
    "    # Guardar\n",
    "    linesdf_lemmas.to_pickle(datawd + \"/linesdf_lemmas/linesdf_lemmas\" + str(indexes[i+1])+ \".pkl\")  \n",
    "    with open(datawd + \"/counter_lemmas.pkl\", 'wb') as outputfile: \n",
    "        pickle.dump(counter, outputfile)\n",
    "        \n",
    "    print( str(perc) , \"% completed\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los datasets creados en el paso anterior para obtener un único dataset con los lemas\n",
    "files = os.listdir(datawd + \"/linesdf_lemmas\")\n",
    "linesdf_lemmas = pd.DataFrame()\n",
    "for f in tqdm(files):\n",
    "    df = pd.read_pickle(datawd + \"/linesdf_lemmas/\" + f)\n",
    "    linesdf_lemmas = pd.concat([linesdf_lemmas, df]).reset_index(drop = True)\n",
    "    \n",
    "print(linesdf.shape[0])\n",
    "print(linesdf_lemmas.shape[0])\n",
    "\n",
    "# liberar memoria :)\n",
    "del(linesdf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = linesdf_lemmas.shape[0] \n",
    "\n",
    "# quitar líneas que quedaron vacías o son sólo espacios (i.e. sólo tenían stopwords o puntuación)\n",
    "linesdf_lemmas = linesdf_lemmas[~linesdf_lemmas.lemmas.str.isspace()].reset_index(drop = True)\n",
    "linesdf_lemmas = linesdf_lemmas[linesdf_lemmas.lemmas != \"\"].reset_index(drop = True)\n",
    "\n",
    "# Ver cuántas quitamos\n",
    "print((linesdf_lemmas.shape[0] - a) / 1e6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUARDAR\n",
    "linesdf_lemmas = linesdf_lemmas.reset_index(drop = True)\n",
    "linesdf_lemmas.to_pickle(datawd + \"/linesdf_lemmas.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='filtro'></a>\n",
    "\n",
    "## Filtrar lemas y líneas\n",
    " 1. [Lemas muy infrecuentes o muy comunes](#filtro-lema-freq-infreq)\n",
    " 2. [Lemas OOV](#filtro-lema-oov)\n",
    " 3. [Líneas y películas outliers en cantidad de lemas](#filtro-lema-outliers) \n",
    " 4. [Lemas en demasiadas o muy pocas películas](#filtro-lema-idf)\n",
    " 5. [Vocabulario final y guardado de datasets](#filtro-final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar datos\n",
    "with open(datawd + \"/counter_lemmas.pkl\", 'rb') as inputfile: \n",
    "    counter = pickle.load(inputfile)\n",
    "linesdf_lemmas = pd.read_pickle(datawd + \"/linesdf_lemmas.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='filtro-lema-freq-infreq'></a>\n",
    "### 1. Lemas muy infrecuentes o muy comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(counter))  # cant. lemas únicos: ~ 478_000\n",
    "plt.hist(counter.values(), bins = 30)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(counter.items(), key=lambda x: x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(counter.items(), key=lambda x: x[1], reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar palabras muy poco frecuentes\n",
    "for m in np.arange(0,51,10):\n",
    "    min_frequency = m  # Umbral de frecuencia mínima a usar\n",
    "    filtered_counter = Counter({word: freq for word, freq in counter.items() if freq >= min_frequency})\n",
    "    print(\"Min. freq.:\", m ,\n",
    "          \"- Number of unique lemmas:\", len(filtered_counter),\n",
    "          \"- Words to remove:\" , len(counter) - len(filtered_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umbrales de mínima y máxima frecuencia\n",
    "min_frequency = 50  \n",
    "max_frequency = 335_000  \n",
    "\n",
    "# Distribución de frecuencias quitando esos lemas (filtrar lemas en el counter)\n",
    "filtered_counter = Counter({word: freq for word, freq in counter.items() if (freq <= max_frequency) & (freq >= min_frequency) })\n",
    "print(\"Number of unique lemmas:\", len(filtered_counter), \n",
    "      \"- Number of lemmas deleted:\", len(counter) - len(filtered_counter))\n",
    "\n",
    "plt.hist(filtered_counter.values(), bins = 30)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='filtro-lema-oov'></a>\n",
    "### 2. Lemas OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-wiki-gigaword-300\")  # Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe a partir del counter filtrado en el paso anterior\n",
    "cdf = pd.DataFrame.from_dict(filtered_counter, orient='index').reset_index()\n",
    "cdf.columns = [\"lemma\", \"counts\"]\n",
    "\n",
    "# Obtener vectores para cada lema\n",
    "cdf[\"WORD_VECTORS\"] = [get_word_vector(word, model = model) for word in tqdm(cdf.lemma)]\n",
    "df_transformed = cdf.WORD_VECTORS.apply(pd.Series) # a columnas\n",
    "df_transformed.columns = [f'dim_{i+1}' for i in range(len(df_transformed.columns))] # agregar dim_ al comienzo de cada columna pertenceciente al vector\n",
    "cdf = pd.concat([ cdf[[\"lemma\",\"counts\"]], df_transformed ], axis = 1)\n",
    "del(df_transformed)\n",
    "\n",
    "# Obtener lemas donde todas las dimensiones son 0 (no tienen representación vectorial)\n",
    "dims = [col for col in cdf.columns if \"dim_\" in col]\n",
    "cdf[\"dimsum\"] = cdf[dims].sum(axis = 1)\n",
    "\n",
    "del(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acerca de las palabras que no aparecen en Glove:\n",
    "- Palabras abreviadas pero comunes: goin' doin' nothin', que fueron eliminadas como stopwords en sus mayoria (bleedin lovin no)\n",
    "- Palabras con errores de tipeo o símbolos raros (ej. nbsp, \\1c&hb0f0f0, uso de i intercambiado con l, falta o repeticion de caracteres)\n",
    "- Insultos: motherfucking, shithead, douchebag\n",
    "- Nombres y palabras poco comunes, tal vez en otro idioma (y solo queremos trabajar con subtítulos en inglés. Aunque eventualmente podría ser bueno ver que se usa otro idioma como predictor de que una película hable de inmigración o no, una limitación actual del trabajo es sólo trabajar con el vocabulario en inglés.\n",
    "- Algunas que sí sería bueno eventualmente corregir: lndian ltalian (con \"l\" en vez de \"I\") covid covid-19\n",
    "\n",
    "Aunque se podría hacer una limpieza refinada, son muy pocas las palabras frecuentes que no son stopwords y parecen contener información útil, con lo cual usaremos Glove como un método de limpieza, filtrando las palabras OOV, y luego podremos usar el mismo modelo como representación vectorial. Una alternativa a probar a furueo sería usar fasttext.\n",
    "\n",
    "Además, quitamos algunos otros símbolos que veo que quedaron como tokens: \"+\", \"f\", \"na\", \"=\" , \">\", \"<\". Debería mejorarse la limpieza previa para que también se quiten dichos símbolos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.counts[cdf.dimsum == 0].hist(bins = 50)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemas y símbolos a quitar\n",
    "remove_others = {\"+\", \"f\", \"na\", \"=\" , \">\", \"<\"}\n",
    "rmv_w2v = set(cdf.lemma[cdf.dimsum == 0]).union(remove_others)\n",
    "print(len(rmv_w2v))\n",
    "\n",
    "# Quitarlos del dataset de lemas y de Counter\n",
    "cdf = cdf[~ cdf.lemma.isin(rmv_w2v)].reset_index(drop = True)\n",
    "filtered_counter2 = Counter({word: freq for word, freq in filtered_counter.items() if word not in rmv_w2v })\n",
    "\n",
    "# Chequear que coinciden cant. lemas en Counter y en dataset\n",
    "print(cdf.shape[0] == len(filtered_counter2))\n",
    "\n",
    "# Cantidad de lemas quitados\n",
    "print(len(filtered_counter2) - len(filtered_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar lemas del dataset a nivel de línea\n",
    "tokenizer = nlp.tokenizer\n",
    "filtered2 = set(filtered_counter2.keys())\n",
    "\n",
    "def filtered_words(text):    \n",
    "    tokens = [t.text for t in tokenizer(text)]\n",
    "    filtered_words = [t for t in tokens if t in filtered2 ]\n",
    "    return (filtered_words)\n",
    "\n",
    "linesdf_lemmas[\"filtered_words2\"] = [filtered_words(x) for x in tqdm(linesdf_lemmas.lemmas, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar cantidad de lemas en cada línea hasta el momento\n",
    "linesdf_lemmas[\"n_words\"] = [len(x) for x in tqdm(linesdf_lemmas.filtered_words2, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitar las líneas que quedan vacías con el nuevo filtro\n",
    "print(np.sum(linesdf_lemmas.n_words == 0))\n",
    "linesdf_lemmas = linesdf_lemmas[linesdf_lemmas.n_words > 0].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='filtro-lema-outliers'></a>\n",
    "### 3. Líneas y películas outliers en cantidad de lemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (a) Lemas por línea (quitar líneas outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesdf_lemmas.n_words.hist()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar las 50 líneas con más cantidad de lemas por líneas como outliers\n",
    "q = 1 - 50 / linesdf_lemmas.shape[0] # calcular cuantil\n",
    "print(q)\n",
    "\n",
    "k = linesdf_lemmas.n_words.quantile(q) # mínima cantidad de lemas\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = linesdf_lemmas[linesdf_lemmas.n_words > k].sort_values(\"n_words\", ascending = False)\n",
    "outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.n_words.hist(bins = 25)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, verificamos que las películas a las que pertenecen las líneas outliers estén OK en general\n",
    "check = linesdf_lemmas[linesdf_lemmas.tconst.isin(outliers.tconst.unique())]\n",
    "check.groupby(\"tconst\").head(5)\n",
    "# Sí, lo están. Simplemente corregiremos las líneas correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar una variable identificadora por película-línea (concatenar tconst y line_number) para facilitar el filtrado manual\n",
    "linesdf_lemmas[\"lineid\"] = linesdf_lemmas.tconst +\"_\" + linesdf_lemmas.line_number.astype(\"str\")\n",
    "linesdf_lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers[\"lineid\"] = outliers.tconst + \"_\" +  outliers.line_number.astype(\"str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se revisó manualmente las líneas seleccionadas. Se decidió:\n",
    "\n",
    "Remover líneas:\n",
    "- Con más de 200 lemas o con 52, 48, 32, 31 o 27 lemas\n",
    "- tt1606197_72, tt8742774_31, tt8742774_43, tt8742774_900, tt9490414_527\n",
    "- Todas las líneas con outliers en las películas: tt0117619, tt1606197, tt3684484, tt5516328\n",
    "\n",
    "Corregir manualmente una línea:\n",
    "- tt1847541_561 debe contener únicamente \"california\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos las correcciones\n",
    "remove_lines = set(outliers.lineid[(outliers.n_words > 200) | \n",
    "                (outliers.n_words.isin([52, 48, 32, 31, 27])) |\n",
    "                (outliers.tconst.isin([\"tt0117619\", \"tt1606197\", \"tt3684484\", \"tt5516328\",\n",
    "                                      \"tt0086203\"])) |\n",
    "                (outliers.lineid.isin([\"tt1606197_72\", \"tt8742774_31\", \"tt8742774_43\", \"tt8742774_900\", \"tt9490414_527\", \n",
    "                                       \"tt8742774_49\", \"tt8742774_18\", \"tt8742774_47\"\n",
    "                                      ]))])\n",
    "\n",
    "linesdf_lemmas.loc[linesdf_lemmas.lineid == \"tt1847541_561\", \"filtered_words2\"] = [\"california\"]\n",
    "linesdf_lemmas.loc[linesdf_lemmas.lineid == \"tt1847541_561\", \"n_words\"] = 1\n",
    "\n",
    "linesdf_lemmas = linesdf_lemmas[~linesdf_lemmas.lineid.isin(remove_lines)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Verificamos que funcionó\n",
    "pd.set_option('display.max_rows',100)\n",
    "outliers = linesdf_lemmas[linesdf_lemmas.n_words > k].sort_values(\"lineid\", ascending = False)\n",
    "outliers # funcionó!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevo histograma de cantidad de lemas por línea\n",
    "linesdf_lemmas.n_words.hist(bins = 20)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (b) Lemas por película (quitar películas outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar también cantidad de caracteres alfabéticos\n",
    "def count_alpha(text):    \n",
    "    alphas = np.sum([t.isalpha() for t in text])\n",
    "    return (alphas)\n",
    "\n",
    "linesdf_lemmas[\"n_alphas\"] = [count_alpha(x) for x in tqdm(linesdf_lemmas.filtered_words2, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largo por película\n",
    "lengthbyfilm = linesdf_lemmas.groupby('tconst').agg({'line': 'count',\n",
    "                                                     'n_words':'sum',\n",
    "                                                     'n_alphas': ['sum', 'mean']})\n",
    "lengthbyfilm.columns = [\"n_words\",  \"n_lines\", \"n_alphas\", \"mean_alpha\"]\n",
    "lengthbyfilm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos algunas películas con 0 caracteres alfabéticos, quitarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmslowalpha = set(lengthbyfilm.index[(lengthbyfilm.mean_alpha < 1.2)])\n",
    "print(len(filmslowalpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthbyfilm = lengthbyfilm[~lengthbyfilm.index.isin(filmslowalpha)]\n",
    "lengthbyfilm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequear las pelis con pocas líneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortl = set(lengthbyfilm.index[(lengthbyfilm.n_lines < 50)])\n",
    "print(len(shortl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# las revisamos manualmente\n",
    "#pd.set_option('display.max_rows',400)\n",
    "#linesdf_lemmas[linesdf_lemmas.tconst.isin(shortl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthbyfilm = lengthbyfilm[~lengthbyfilm.index.isin(shortl)]\n",
    "lengthbyfilm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthbyfilm.hist(figsize = (20,5), bins = 50 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitamos ambos conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films_to_remove = filmslowalpha.union(shortl)\n",
    "print(len(films_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesdf_lemmas = linesdf_lemmas[~linesdf_lemmas.tconst.isin(films_to_remove )].reset_index(drop = True)\n",
    "linesdf_lemmas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='filtro-lema-idf'></a>\n",
    "### 4. Lemas en demasiadas o muy pocas películas\n",
    "Basándonos en la IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar la matriz idf \n",
    "\n",
    "## partir del texto agrupado\n",
    "df = linesdf_lemmas.groupby('tconst',as_index = False)[\"filtered_words2\"].agg(list)\n",
    "df[\"filtered_words2\"] = [[item for sublist in l for item in sublist] for l in df.filtered_words2]\n",
    "df[\"text_clean\"] = [\" \".join(x) for x in df.filtered_words2]\n",
    "\n",
    "## calcular matrices usando sklearn\n",
    "pipe = Pipeline([('count', CountVectorizer()),\n",
    "                  ('tfid', TfidfTransformer())]).fit(df.text_clean)\n",
    "vocab = pipe['count'].vocabulary_\n",
    "ivocab = {v: k for k, v in vocab.items()}\n",
    "idfs = pipe['tfid'].idf_\n",
    "idfs     = pd.DataFrame(idfs)\n",
    "    \n",
    "# cambiar nombre de filas a los lemas en IDF\n",
    "idfs = idfs.rename(index = ivocab)\n",
    "idfs.columns = [\"idf\"]\n",
    "idfs = idfs.sort_values(\"idf\") # Aquellos con menor IDF son los lemas que aparecen en más películas\n",
    "\n",
    "idfs.hist(bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = set(idfs.head(25).index) # Son principalmente verbos\n",
    "idfs.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon_words = idfs[idfs.idf >= 8] # mostly names\n",
    "print(len(uncommon_words ))\n",
    "#pd.set_option('display.max_rows',len(uncommon_words ))\n",
    "#uncommon_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quitar ambos conjuntos\n",
    "words_to_remove = set(uncommon_words.index).union(common_words)\n",
    "len(words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesdf_lemmas.reset_index(drop = True, inplace = True)\n",
    "linesdf_lemmas[\"filtered_words3\"] = np.nan\n",
    "linesdf_lemmas[\"filtered_words3\"] = [[w for w in row if w not in words_to_remove] for row in tqdm(linesdf_lemmas.filtered_words2, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesdf_lemmas[\"n_words\"] = [len(x) for x in tqdm(linesdf_lemmas.filtered_words3, position=0, leave=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesdf_lemmas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linesdf_lemmas = linesdf_lemmas[linesdf_lemmas.n_words > 0].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='filtro-final'></a>\n",
    "### 5. Vocabulario final y guardado de datasets\n",
    "Quitar los lemas infrecuentes luego del filtrado previo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la cantidad de apariciones\n",
    "counter = Counter()\n",
    "    for row in linesdf_lemmas.filtered_words3:\n",
    "        for lemma in row:\n",
    "            counter[lemma] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vocabulario con lemas que aparecen al menos 50 veces\n",
    "vocab = Vocab(\n",
    "    counter,\n",
    "    min_freq=50, \n",
    "    specials=[\"<unk>\", \"<pad>\"],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "stoi = vocab.get_stoi()\n",
    "\n",
    "UNK_IDX = stoi[\"<unk>\"]\n",
    "PAD_IDX = stoi[\"<pad>\"]\n",
    "\n",
    "len(vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_lemmas(example): # lemas que no son UNK\n",
    "    final_lemmas  = [t  for t in example if stoi.get(t, UNK_IDX) != UNK_IDX] \n",
    "    return final_lemmas\n",
    "\n",
    "def to_vocab(example): # token ids\n",
    "    token_ids = [stoi.get(t, UNK_IDX)  for t in example] \n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byfilm = pd.DataFrame() # create dataset at the film level\n",
    "\n",
    "# Ir guardando en datasets a nivel de línea separados (si no es muy grande para leer en memoria)\n",
    "n = len(set(linesdf_lemmas.tconst))\n",
    "step = int(round(byfilm.shape[0] / 20))\n",
    "indexes = np.arange(0 , n-step , step).tolist() +  [n]\n",
    "\n",
    "for i in tqdm(range(len(indexes)-1), position=0, leave=True):\n",
    "    \n",
    "    # Quedarse con un subconjunto de películas\n",
    "    films = set(byfilm.index[indexes[i]:  indexes[i+1]])\n",
    "    df = linesdf_lemmas[linesdf_lemmas.tconst.isin(films)]\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        df[\"final_lemmas\"] = list(map(final_lemmas, df.filtered_words3)) # lemas que no son UNK\n",
    "        df[\"token_ids\"]    = list(map(to_vocab, df.final_lemmas)) # token ids\n",
    "        df[\"n_tokens\"]     =  list(map( lambda x: len(x), df.final_lemmas))\n",
    "\n",
    "        df.drop(columns = [\"lemmas\", \"filtered_words2\", \"n_words\", \"n_alphas\", \"filtered_words3\"],\n",
    "                   inplace = True)\n",
    "\n",
    "        df = df[df.n_tokens > 0].reset_index(drop = True)\n",
    "\n",
    "        df.to_pickle(datawd + \"/linesdf_lemmas_filtered/linesdf_lemmas_filtered\"+ str(indexes[i+1]) + \".pkl\")\n",
    "\n",
    "   \n",
    "    \n",
    "    # Dataset agrupando por película\n",
    "    df = df.groupby('tconst',as_index = False).agg({\"final_lemmas\" : list,\n",
    "                                                    \"line\": \"count\" ,\n",
    "                                                   \"token_ids\": list,\n",
    "                                                   \"n_tokens\": sum})\n",
    "    df[\"final_lemmas\"] = [[item for sublist in l for item in sublist] for l in df.final_lemmas]\n",
    "    df[\"token_ids\"] = [[item for sublist in l for item in sublist] for l in df.token_ids]\n",
    "    \n",
    "    byfilm = pd.concat([byfilm, df])\n",
    "\n",
    "del(df)\n",
    "byfilm = byfilm.rename(columns={'line': 'n_lines'})\n",
    "byfilm.to_pickle(datawd + \"/linesdf_lemmas_filtered/byfilm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(byfilm.shape)\n",
    "byfilm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab\n",
    "with open(datawd + \"/vocab.pkl\", 'wb') as outputfile: # 'wb' means Write Binary. Instead, use 'rb' to read\n",
    "    pickle.dump(vocab, outputfile)\n",
    "\n",
    "# save stoi\n",
    "with open(datawd + \"/stoi.pkl\", 'wb') as outputfile: # 'wb' means Write Binary. Instead, use 'rb' to read\n",
    "    pickle.dump(stoi, outputfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregamos las nuevas variables al dataset maestro de contenido de subtítulos\n",
    "master = pd.read_pickle(datawd + \"/master_subt_content.pkl\")  \n",
    "master[\"file_type\"] = [fname[-4:] for fname in master.subt]\n",
    "master = master.merge(byfilm,\n",
    "                     on = \"tconst\",\n",
    "                     how = \"left\",\n",
    "                     indicator = True,\n",
    "                     validate = \"1:1\")\n",
    "del(byfilm)\n",
    "\n",
    "master[\"in_cleaned\"] = 0\n",
    "master.loc[master._merge == \"both\", \"in_cleaned\"] = 1\n",
    "master.drop(columns = [\"_merge\"], inplace = True)\n",
    "master.head(2)\n",
    "\n",
    "master.to_pickle(datawd + \"/master_subt_content_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos una versión más liviana\n",
    "print(master.columns)\n",
    "\n",
    "master[[\"tconst\",\n",
    "       \"in_cleaned\",\n",
    "        \"final_lemmas\",\n",
    "        \"main\",\n",
    "        \"before2000\",\n",
    "        \"just_migra\"   ]].to_pickle(datawd + \"/master_subt_content_cleaned_lite.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
