{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping - subtítulos YIFY\n",
    "\n",
    "Este Notebook scrapea subtítulos de YIFI, guarda los archivos, carga el texto como variable de un dataframe, quita duplicados. Pasos:\n",
    "\n",
    "1. [Descarga subtítulos](#download)\n",
    "2. [Carga de subtítulos como variable de texto](#upload)\n",
    "3. [Duplicados](#dup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías \n",
    "\n",
    "## Módulos generales\n",
    "from libraries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completar con directorios \n",
    "gitwd = \"\"\n",
    "datawd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='download'></a>\n",
    "## Descarga subtítulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de películas a buscar\n",
    "titles = pd.read_csv(datawd + \"/titles_restricted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(datawd)\n",
    "files = os.listdir()\n",
    "\n",
    "if 'titles_subt.csv' in files: # ya habíamos comenzado la descarga y se interrumpió\n",
    "    titles_subt = pd.read_csv('titles_subt.csv')\n",
    "    urls = titles_subt.urls\n",
    "    ns = list(titles_subt.index[titles_subt.subt.isnull()]) # el primer nulo en subt es donde dejamos\n",
    "\n",
    "if 'titles_subt.csv' not in files:\n",
    "    titles_subt = titles[[\"tconst\"]]\n",
    "    titles_subt.loc[:,\"subt\"] = \"\" # guardar nombre de archivos de subtítulos o \"no tiene\" si no encontramos subtítulos\n",
    "    urls = ['https://yts-subs.com/movie-imdb/'+t for t in titles.tconst] \n",
    "    titles_subt.loc[:,\"urls\"] = urls # iterar por las páginas de subtítulos y descargarlos\n",
    "    ns = list(titles_subt.index) # desde el inicio\n",
    "\n",
    "    \n",
    "# subtítulos ya descargados\n",
    "os.chdir(datawd+\"/subts\")\n",
    "already_downloaded = os.listdir()\n",
    "\n",
    "# cada 1000 guardamos una copia del dataset\n",
    "time_to_save = list(np.arange(1000, len(urls), 1000))\n",
    "print(len(time_to_save))\n",
    "\n",
    "# deshabilitar SettingWithCopyWarning \n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "print(ns[:5])\n",
    "\n",
    "# loop\n",
    "for n in tqdm(ns):\n",
    "    \n",
    "    r = requests.get(urls[n])  \n",
    "    html = r.content.decode('utf-8')\n",
    "    bhtml = bs4.BeautifulSoup(html) \n",
    "    links = [d.get('href') for d in bhtml.find_all('a',attrs = {'class':\"subtitle-download\"})]\n",
    "   \n",
    "    if not links: # está vacía la lista de links (no tiene subtítulo)\n",
    "        titles_subt.loc[n, \"subt\"] = 'no tiene'\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            english = [not not re.search('english',l) for l in links]\n",
    "            link = np.array(links)[english][0].split('/')[-1]\n",
    "            \n",
    "            descargar = 'https://yifysubtitles.org/subtitle/' + link + '.zip' # el link del zip para la descarga empieza en https://yifysubtitles.org/subtitle y termina en .zip\n",
    "            request = requests.get(descargar)\n",
    "            file = zipfile.ZipFile(BytesIO(request.content)) # archivo a descargar\n",
    "            \n",
    "            name = titles.tconst[n] +  file.namelist()[0][-4:] # nombre de archivo a usar\n",
    "            titles_subt.loc[n, \"subt\"] = name\n",
    "\n",
    "            if name not in already_downloaded: # descargar aquellos que no se hayan descargado todavía\n",
    "                file.extract(member = file.namelist()[0], path = sdir + \"/temp\" ) # lo descarga al path (carpeta temporaria con otro nombre)\n",
    "                os.rename(sdir + \"/temp/\" + file.namelist()[0], \n",
    "                          sdir + \"/\" + name) # renombrar luego de guardar y mover a la carpeta permanente\n",
    "                os.rmdir( sdir + \"/temp\" ) # eliminar carpeta temporaria\n",
    "                \n",
    "        except:\n",
    "            titles_subt.loc[n, \"subt\"] = 'no tiene' # a veces dice 'no se encuentra el link en nuestros servidores'\n",
    "\n",
    "    if n in time_to_save: # every 1000 subts, lets save the dataset\n",
    "        titles_subt.to_csv(datawd + \"/titles_master.csv\", index = False)\n",
    "\n",
    "# when finished, save\n",
    "titles_subt.to_csv(datawd + \"/titles_master.csv\", index = False)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='upload'></a>\n",
    "## Películas con subtítulos y duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Porcentaje de películas sin subtítulos: \",\n",
    "      np.mean(titles_subt.subt == \"no tiene\") * 100)\n",
    "\n",
    "master_subt_content = titles_subt[titles_subt.subt != \"no tiene\"].reset_index(drop = True)\n",
    "\n",
    "print(\"Nombres de archivo de subtítulos duplicados:\",np.sum(master_subt_content.subt.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable con los nombres completos de los archivos (incluyendo directorio)\n",
    "master_subt_content[\"files\"] = [datawd + \"/subts/\" + master_subt_content.subt[i] for i in range(master_subt_content.shape[0])]\n",
    "\n",
    "# Leer contenido para cada película \n",
    "subtitulos = []\n",
    "\n",
    "for f in tqdm(range(master_subt_content.shape[0])):\n",
    "    try:\n",
    "        subtitulos.append(' '.join(open(master_subt_content.files[f], \"r\").readlines()))\n",
    "    except: \n",
    "        try:\n",
    "            subtitulos.append(' '.join(open(master_subt_content.files[f], \"r\", encoding='utf-8').readlines()))\n",
    "        except:\n",
    "            try:\n",
    "                subtitulos.append(json.load(' '.join(open(master_subt_content.files[f], \"r\", encoding='utf-8'))))\n",
    "            except:\n",
    "                subtitulos.append('no abre')\n",
    "                \n",
    "master_subt_content[\"s\"] = subtitulos \n",
    "\n",
    "print(\"No abre el archivo de subtítulos:\", np.sum(master_subt_content.s == \"no abre\"))\n",
    "master_subt_content = master_subt_content[master_subt_content.s != \"no abre\"].reset_index(drop = True)\n",
    "print(master_subt_content.shape) \n",
    "\n",
    "# Guardar\n",
    "master_subt_content.to_pickle(datawd + \"/master_subt_content.pkl\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='dup'></a>\n",
    "## Duplicados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay casos donde tenemos duplicados por errores en YIFI, donde la película se llama muy parecido, y cargan el mismo archivo de subtítulos para dos películas distintas. Los descartamos por no saber a qué película corresponden realmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_subt_content[\"dup\"] = master_subt_content.s.duplicated(keep = False)\n",
    "print(f\"Cantidad de subtítulos con contenido duplicado: {np.sum(master_subt_content.dup)}\")\n",
    "\n",
    "dups = set(master_subt_content.tconst[master_subt_content.dup])\n",
    "master_subt_content = master_subt_content[~master_subt_content.dup].reset_index(drop = True)\n",
    "\n",
    "# Guardar nuevamente\n",
    "master_subt_content.to_pickle(datawd + \"/master_subt_content.pkl\")     \n",
    "\n",
    "\n",
    "# agregar indicación de duplicados en el master para saber que no debemos contarslos\n",
    "master.subt[master.tconst.isin(dups)] = \"dup\"\n",
    "\n",
    "# save again\n",
    "master.to_csv(datawd + \"/titles_master.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
